{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBuehTnFnkFg",
        "outputId": "9c4c83d7-71c1-4604-a68b-9d2d0ede8b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\",\"RDDDemo\")\n",
        "rdd = sc.parallelize([1,2,3,4,5])\n",
        "rdd2 = rdd.map(lambda x : x ** x)\n",
        "rdd2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkO4rAsjoKnU",
        "outputId": "1ce9b48f-5286-4fac-f6dd-949db9c39717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 27, 256, 3125]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "\n",
        "\n",
        "sc = SparkContext(\"local\", \"PySparkWordCount\")\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"PySpark allows data processing using Python and Apache Spark\",\n",
        "    \"Transformations in PySpark are lazy and optimized at runtime\",\n",
        "    \"PySpark is widely used for big data analysis and machine learning tasks\"\n",
        "]\n",
        "rdd = sc.parallelize(sentences)\n",
        "\n",
        "\n",
        "stopwords = {\"is\", \"a\", \"the\", \"and\", \"in\", \"for\", \"at\", \"are\", \"using\", \"of\"}\n",
        "\n",
        "\n",
        "words = rdd.flatMap(lambda sentence: sentence.lower().split())\n",
        "\n",
        "\n",
        "filtered_words = words.filter(lambda word: word not in stopwords)\n",
        "\n",
        "\n",
        "word_pairs = filtered_words.map(lambda word: (word, 1))\n",
        "print(word_pairs.collect())\n",
        "\n",
        "\n",
        "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
        "sc.stop()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPYdP5ty137N",
        "outputId": "77830a04-c1a4-4a34-c00b-3f3f964260e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('pyspark', 1), ('allows', 1), ('data', 1), ('processing', 1), ('python', 1), ('apache', 1), ('spark', 1), ('transformations', 1), ('pyspark', 1), ('lazy', 1), ('optimized', 1), ('runtime', 1), ('pyspark', 1), ('widely', 1), ('used', 1), ('big', 1), ('data', 1), ('analysis', 1), ('machine', 1), ('learning', 1), ('tasks', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MqI6w28522QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "\n",
        "sc = SparkContext(\"local\", \"AverageScore\")\n",
        "\n",
        "data = [(\"Alice\", 80), (\"Bob\", 90), (\"Alice\", 70), (\"Bob\", 85), (\"Charlie\", 60)]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "score_pairs = rdd.map(lambda x: (x[0], (x[1], 1)))\n",
        "\n",
        "score_totals = score_pairs.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "\n",
        "average_scores = score_totals.map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
        "\n",
        "\n",
        "print(\"Average Scores:\")\n",
        "for name, avg in average_scores.collect():\n",
        "    print(f\"{name}: {avg:.2f}\")\n",
        "sc.stop()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llkn-Q9m6r-R",
        "outputId": "469c1beb-ae60-454f-f143-3ade77455380"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Scores:\n",
            "Alice: 75.00\n",
            "Bob: 87.50\n",
            "Charlie: 60.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "\n",
        "sc.stop()\n",
        "sc = SparkContext(\"local\", \"TopFrequentNumbers\")\n",
        "\n",
        "rdd = sc.parallelize([5, 3, 4, 5, 2, 3, 5, 3, 4])\n",
        "\n",
        "number_ones = rdd.map(lambda x: (x, 1))\n",
        "\n",
        "number_counts = number_ones.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "count_number = number_counts.map(lambda x: (x[1], x[0]))\n",
        "\n",
        "sorted_counts = count_number.sortByKey(ascending=False)\n",
        "\n",
        "top3 = sorted_counts.take(3)\n",
        "print(top3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8CTCiT9E5D_",
        "outputId": "56c3a89d-7926-42f2-aa5f-e4137c52d325"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(3, 5), (3, 3), (2, 4)]\n"
          ]
        }
      ]
    }
  ]
}